{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.preprocessing.text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#导入Keras中的卷积工具\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from keras.models import Sequential  #基础的Keras神经网络模型\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from keras.layers import Dense, Dropout, Activation #模型中常用的层对象\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# from keras.layers import Conv1D,GlobalMaxPooling1D #卷积层和池化\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  Input, Dense,Embedding,Conv1D,MaxPooling1D,LSTM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "#导入Keras中的卷积工具\n",
    "# from keras.models import Sequential  #基础的Keras神经网络模型\n",
    "# from keras.layers import Dense, Dropout, Activation #模型中常用的层对象\n",
    "# from keras.layers import Conv1D,GlobalMaxPooling1D #卷积层和池化\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import  Input, Dense,Embedding,Conv1D,MaxPooling1D,LSTM\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "df = pd.read_csv('./data.csv')\n",
    "df=df[['text','location', 'country']]\n",
    "\n",
    "#print(\"在 text 列中总共有 %d 个空值.\" % df['text'].isnull().sum())\n",
    "#print(\"在 location 列中总共有 %d 个空值.\" % df['location'].isnull().sum())\n",
    "df[df.isnull().values==True]\n",
    "df = df[pd.notnull(df['text'])]\n",
    "#print(df)\n",
    "\n",
    "# df.sample(2)\n",
    "texts = [[word for word in jieba.cut(document)] for document in df['text']]\n",
    "\n",
    "print(texts)\n",
    "# 标签编码\n",
    "df['country_id'] = df['country'].factorize()[0]\n",
    "df['location_id'] = df['location'].factorize()[0]\n",
    "cat_id_df = df[['country', 'country_id']].drop_duplicates().sort_values('country_id').reset_index(drop=True)\n",
    "cat_id_df = df[['location', 'location_id']].drop_duplicates().sort_values('location_id').reset_index(drop=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['中', '柬', '在', '南海', '举行', '联合', '军演'], ['美菲于', '2008', '年', '9', '月', '10', '日', '在', '黄海', '举行', '联合', '军演'], ['美韩', '在', '日本海', '举行', '联合', '军演'], ['中泰', '在', '南海', '举行', '联合', '军演'], ['中', '柬', '在', '南海', '举行', '联合', '军演'], ['日', '韩', '在', '日本海', '举行', '联合', '军演'], ['中泰', '在', '南海', '举行', '联合', '军演'], ['中俄于', '2023', '年', '9', '月', '10', '日', '在', '东海', '举行', '联合', '军演'], ['新加坡', '在', '吕宋岛', '举行', '军演'], ['新加坡', '在', '吕宋岛', '举行', '军演'], ['中', '缅', '在', '珊瑚', '海', '举行', '联合', '军演'], ['中俄于', '2023', '年', '9', '月', '10', '日', '在', '东海', '举行', '联合', '军演'], ['中老于', '2013', '年', '2', '月', '10', '日', '在', '东海', '举行', '联合', '军演'], ['美日于', '2023', '年', '9', '月', '10', '日', '在', '黄海', '举行', '联合', '军演'], ['中', '缅', '在', '珊瑚', '海', '举行', '联合', '军演'], ['日', '韩', '在', '日本海', '举行', '联合', '军演']] [['美日于', '2023', '年', '9', '月', '10', '日', '在', '黄海', '举行', '联合', '军演'], ['中老于', '2013', '年', '2', '月', '10', '日', '在', '东海', '举行', '联合', '军演'], ['美菲于', '2008', '年', '9', '月', '10', '日', '在', '黄海', '举行', '联合', '军演'], ['美韩', '在', '日本海', '举行', '联合', '军演']] 8     3\n",
      "5     0\n",
      "11    1\n",
      "3     3\n",
      "18    3\n",
      "16    1\n",
      "13    3\n",
      "2     2\n",
      "9     5\n",
      "19    5\n",
      "4     4\n",
      "12    2\n",
      "7     2\n",
      "10    0\n",
      "14    4\n",
      "6     1\n",
      "Name: location_id, dtype: int64 0     0\n",
      "17    2\n",
      "15    0\n",
      "1     1\n",
      "Name: location_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(texts,df['location_id'],test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_seq_mat [[0 0 0 ... 2 4 3]\n",
      " [0 0 0 ... 2 4 3]\n",
      " [0 0 0 ... 2 4 3]\n",
      " ...\n",
      " [0 0 0 ... 2 4 3]\n",
      " [0 0 0 ... 2 4 3]\n",
      " [0 0 0 ... 2 4 3]] (16, 100)\n",
      "y_train_seq [[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]] (16, 6)\n"
     ]
    }
   ],
   "source": [
    "max_words = 100\n",
    "max_len = 100\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(texts)\n",
    "\n",
    "train_seq = tok.texts_to_sequences(X_train)\n",
    "test_seq = tok.texts_to_sequences(X_test)\n",
    "\n",
    "train_seq_mat = pad_sequences(train_seq,maxlen=max_len)\n",
    "test_seq_mat = pad_sequences(test_seq,maxlen=max_len)\n",
    "\n",
    "# 标签独热编码\n",
    "ohe = OneHotEncoder()\n",
    "y_train_seq = ohe.fit_transform(np.array(Y_train).reshape(-1,1)).toarray()\n",
    "y_test_seq = ohe.transform(np.array(Y_test).reshape(-1,1)).toarray()\n",
    "\n",
    "\n",
    "# print(texts)\n",
    "# print(train_seq)\n",
    "print('train_seq_mat', train_seq_mat, train_seq_mat.shape)\n",
    "# print('test_seq', test_seq)\n",
    "# print('test_seq_mat', test_seq_mat)\n",
    "print('y_train_seq', y_train_seq, y_train_seq.shape)\n",
    "# print('y_test_seq', y_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">606</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ inputs (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m12,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m12,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m53,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m606\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,158</span> (320.93 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m82,158\u001b[0m (320.93 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,158</span> (320.93 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m82,158\u001b[0m (320.93 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975ms/step - accuracy: 0.2500 - loss: 1.7900 - val_accuracy: 0.0000e+00 - val_loss: 1.7853\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2500 - loss: 1.7821 - val_accuracy: 0.0000e+00 - val_loss: 1.7807\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 1.7813 - val_accuracy: 0.0000e+00 - val_loss: 1.7775\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2500 - loss: 1.7788 - val_accuracy: 0.0000e+00 - val_loss: 1.7732\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 1.7683 - val_accuracy: 0.0000e+00 - val_loss: 1.7707\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 1.7635 - val_accuracy: 0.0000e+00 - val_loss: 1.7692\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2500 - loss: 1.7548 - val_accuracy: 0.0000e+00 - val_loss: 1.7671\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2500 - loss: 1.7529 - val_accuracy: 0.0000e+00 - val_loss: 1.7651\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2500 - loss: 1.7411 - val_accuracy: 0.0000e+00 - val_loss: 1.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 6\n",
    "## 定义CNN-LSTM模型\n",
    "inputs = Input(name='inputs',shape=[max_len])\n",
    "## Embedding(词汇表大小,batch大小,每个新闻的词长)\n",
    "layer = Embedding(max_words+1,128,input_shape=max_len)(inputs)\n",
    "layer = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(layer)\n",
    "layer = MaxPooling1D(pool_size=2)(layer)\n",
    "layer = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(layer)\n",
    "layer = MaxPooling1D(pool_size=2)(layer)\n",
    "layer = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(layer)\n",
    "layer = Dense(num_classes, activation='softmax')(layer)\n",
    "model = Model(inputs=inputs,outputs=layer)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_seq_mat,y_train_seq,batch_size=128,epochs=10,\n",
    "                      validation_data=(test_seq_mat,y_test_seq),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.000001),TensorBoard(log_dir='./log')]\n",
    "                        ## 当val-loss不再降低时停止训练\n",
    "                     )\n",
    "# # 保存模型\n",
    "model.save('CNN-LSTM-10.h5')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_texts [['美日于', '2023', '年', '9', '月', '10', '日', '在', '黄海', '举行', '联合', '军演', '，', '台湾', '参与', '其中'], ['中老', '在', '东海', '举行', '联合', '军演'], ['中', '柬', '在', '南海', '举行', '联合', '军演'], ['新加坡', '在', '吕宋岛', '举行', '军演']]\n",
      "predict_seq [[16, 10, 6, 9, 7, 8, 5, 1, 11, 2, 4, 3], [1, 13, 2, 4, 3], [15, 29, 1, 14, 2, 4, 3], [30, 1, 31, 2, 3]]\n",
      "predict_seq_mat [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16 10  6  9  7  8  5  1\n",
      "  11  2  4  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "  13  2  4  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15 29  1\n",
      "  14  2  4  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30\n",
      "   1 31  2  3]]\n"
     ]
    }
   ],
   "source": [
    "predict_df = pd.read_csv('./predict.csv')\n",
    "predict_texts = [[word for word in jieba.cut(document)] for document in predict_df['text']]\n",
    "predict_seq = tok.texts_to_sequences(predict_texts)\n",
    "predict_seq_mat = pad_sequences(predict_seq,maxlen=max_len)\n",
    "print('predict_texts', predict_texts)\n",
    "print('predict_seq', predict_seq)\n",
    "print('predict_seq_mat', predict_seq_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "[2 3 3 3] [[0.16508111 0.16590548 0.17152835 0.1705709  0.16230057 0.16461352]\n",
      " [0.16453905 0.16629338 0.1699449  0.1714949  0.16261694 0.16511084]\n",
      " [0.16339193 0.16595112 0.16931581 0.17282142 0.16349532 0.16502431]\n",
      " [0.16411512 0.16633752 0.16880742 0.1722457  0.16309933 0.16539495]] 0    美日于2023年9月10日在黄海举行联合军演，台湾参与其中\n",
      "1                      中老在东海举行联合军演\n",
      "2                      中柬在南海举行联合军演\n",
      "3                      新加坡在吕宋岛举行军演\n",
      "Name: text, dtype: object\n",
      "待分类文本： 美日于2023年9月10日在黄海举行联合军演，台湾参与其中\n",
      "地点分类结果： 东海\n",
      "待分类文本： 中老在东海举行联合军演\n",
      "地点分类结果： 南海\n",
      "待分类文本： 中柬在南海举行联合军演\n",
      "地点分类结果： 南海\n",
      "待分类文本： 新加坡在吕宋岛举行军演\n",
      "地点分类结果： 南海\n"
     ]
    }
   ],
   "source": [
    "# 导入已经训练好的模型\n",
    "model = load_model('CNN-LSTM-10.h5')\n",
    "predict_pre = model.predict(predict_seq_mat)\n",
    "pred = np.argmax(predict_pre, axis=1)\n",
    "\n",
    "print(pred, predict_pre, predict_df['text'])\n",
    "labels = ['黄海', '日本海', '东海', '南海', '吕宋岛']\n",
    "count=0\n",
    "for i in pred:\n",
    "    print('待分类文本：', predict_df['text'][count])\n",
    "    print('地点分类结果：', labels[i])\n",
    "    count = count + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
