{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量编码\n",
    "# word_model = word2vec.Word2Vec(texts,vector_size=100,min_count=3,window=6,hs=1)\n",
    "word_model = word2vec.Word2Vec(texts, hs=1,min_count=1,window=3,vector_size=10)\n",
    "# word_model.save('word_model')  # 保存模型\n",
    "# word_model = word2vec.Word2Vec.load('word_model')  # 加载模型\n",
    "# print(model.wv.similarity('南海', '日本海'))\n",
    "# print(model.wv['南海'])\n",
    "# for val in model.wv.similar_by_word(\"南海\", topn=10):\n",
    "#     print(val[0], val[1])\n",
    "for token in texts:\n",
    "    print(word_model.wv[token])\n",
    "\n",
    "print(word_model.wv)\n",
    "# 直接词向量相加求平均\n",
    "def fea_sentence(list_w):\n",
    "    n0 = np.array([0. for i in range(2)], dtype=np.float32)\n",
    "    for i in list_w:\n",
    "        n0 += i\n",
    "    fe = n0 / len(list_w)\n",
    "    fe = fe.tolist()\n",
    "    return fe\n",
    "\n",
    "# 向量表\n",
    "vectorized_data=[]\n",
    "for text in texts:\n",
    "    sample_vecs = []\n",
    "    for token in text:\n",
    "        try:\n",
    "            sample_vecs.append(word_model.wv[token])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    vectorized_data.append(fea_sentence(sample_vecs))\n",
    "x_train=vectorized_data\n",
    "y_train=df[\"country_id\"]\n",
    "print('x_train', x_train)\n",
    "print('y_train', y_train)\n",
    "print('x_train', np.shape(x_train))\n",
    "print('y_train', np.shape(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 模型\n",
    "maxlen=400\n",
    "batch_size=32 #在后向传播误差和更新权重前，向网络传入的样本数量\n",
    "embedding_dims=2 #传入卷积神经网络中词条向量的长度\n",
    "filters=250 #要训练的卷积核的数量\n",
    "kernel_size=3 #卷积核大小：每个卷积核将是一个矩阵：embedding_dims*kernel_size\n",
    "hidden_dims=250 #在普通的前馈网络中传播链端点的神经元的数量\n",
    "epochs=1 #整个训练集在网络中的传入次数\n",
    "#定义模型\n",
    "model = Sequential()\n",
    "model.add(Conv1D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    input_shape=(maxlen,embedding_dims)\n",
    "))\n",
    "#池化\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#带dropout的全连接层\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "#漏斗funnel\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "#编译CNN\n",
    "model.compile(loss='sparse_categorical_crossentropy', #多分类的损失函数\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "#categorical变量的输出层\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('sigmoid'))\n",
    "#训练CNN\n",
    "model.fit(\n",
    " x_train_seq,\n",
    " y_train_seq,\n",
    " batch_size=batch_size,\n",
    " epochs=epochs,\n",
    " #  validation_split = 0.2\n",
    " # 训练集的20%用作验证集\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "model_structure=model.to_json()\n",
    "with open(\"cnn_model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights('model_CNN_text.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    df = pd.read_csv('./data.csv')\n",
    "    df=df[['text','location', 'country']]\n",
    "\n",
    "    df[df.isnull().values==True]\n",
    "    df = df[pd.notnull(df['text'])]\n",
    "\n",
    "\n",
    "    df['text_id'] = df['text'].factorize()[0]\n",
    "    cat_id_df = df[['text', 'text_id']].drop_duplicates().sort_values('text_id').reset_index(drop=True)\n",
    "    cat_to_id = dict(cat_id_df.values)\n",
    "    id_to_cat = dict(cat_id_df[['text_id', 'text']].values)\n",
    "\n",
    "\n",
    "    texts = [[word for word in jieba.cut(document)]for document in df['text']]\n",
    "    for txt in texts:\n",
    "        sample_vecs = []\n",
    "        for token in txt:\n",
    "            try:\n",
    "                sample_vecs.append(model.wv[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    padded = np.reshape(sample_vecs,maxlen=maxlen)\n",
    "    pred = model.predict(padded)\n",
    "    cat_id= pred.argmax(axis=1)[0]\n",
    "    return cat_id_df[cat_id_df.cat_id==cat_id]['cat'].values[0]\n",
    "\n",
    "predict(\"衣服真的很大很大，但是颜色好看，字母也很有质感，喜欢喜欢\")\n",
    "predict(\"美韩在日本海举行联合军演\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
