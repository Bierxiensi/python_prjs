{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用句法依存分析抽取事实三元组\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyltp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/bierxiensi/Desktop/python_prjs/Proj2/index.ipynb 单元格 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj2/index.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj2/index.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj2/index.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyltp\u001b[39;00m \u001b[39mimport\u001b[39;00m Segmentor, Postagger, Parser, NamedEntityRecognizer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj2/index.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m正在加载LTP模型... ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj2/index.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m segmentor \u001b[39m=\u001b[39m Segmentor()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyltp'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\"\"\"\n",
    "文本中事实三元组抽取\n",
    "python *.py input.txt output.txt begin_line end_line\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"tianwen jiang\"\n",
    "\n",
    "# Set your own model path\n",
    "MODELDIR=\"/data/ltp/ltp-models/3.3.0/ltp_data\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    " \n",
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer\n",
    "\n",
    "print(\"正在加载LTP模型... ...\")\n",
    "\n",
    "segmentor = Segmentor()\n",
    "segmentor.load(os.path.join(MODELDIR, \"cws.model\"))\n",
    "\n",
    "postagger = Postagger()\n",
    "postagger.load(os.path.join(MODELDIR, \"pos.model\"))\n",
    "\n",
    "parser = Parser()\n",
    "parser.load(os.path.join(MODELDIR, \"parser.model\"))\n",
    "\n",
    "recognizer = NamedEntityRecognizer()\n",
    "recognizer.load(os.path.join(MODELDIR, \"ner.model\"))\n",
    "\n",
    "#labeller = SementicRoleLabeller()\n",
    "#labeller.load(os.path.join(MODELDIR, \"srl/\"))\n",
    "\n",
    "print(\"加载模型完毕。\")\n",
    "\n",
    "in_file_name = \"input.txt\"\n",
    "out_file_name = \"output.txt\"\n",
    "begin_line = 1\n",
    "end_line = 0\n",
    "\n",
    "if len(sys.argv) > 1:\n",
    "    in_file_name = sys.argv[1]\n",
    "\n",
    "if len(sys.argv) > 2:\n",
    "    out_file_name = sys.argv[2]\n",
    "\n",
    "if len(sys.argv) > 3:\n",
    "    begin_line = int(sys.argv[3])\n",
    "\n",
    "if len(sys.argv) > 4:\n",
    "    end_line = int(sys.argv[4])\n",
    "\n",
    "def extraction_start(in_file_name, out_file_name, begin_line, end_line):\n",
    "    \"\"\"\n",
    "    事实三元组抽取的总控程序\n",
    "    Args:\n",
    "        in_file_name: 输入文件的名称\n",
    "        #out_file_name: 输出文件的名称\n",
    "        begin_line: 读文件的起始行\n",
    "        end_line: 读文件的结束行\n",
    "    \"\"\"\n",
    "    in_file = open(in_file_name, 'r')\n",
    "    out_file = open(out_file_name, 'a')\n",
    "    \n",
    "    line_index = 1\n",
    "    sentence_number = 0\n",
    "    text_line = in_file.readline()\n",
    "    while text_line:\n",
    "        if line_index < begin_line:\n",
    "            text_line = in_file.readline()\n",
    "            line_index += 1\n",
    "            continue\n",
    "        if end_line != 0 and line_index > end_line:\n",
    "            break\n",
    "        sentence = text_line.strip()\n",
    "        if sentence == \"\" or len(sentence) > 1000:\n",
    "            text_line = in_file.readline()\n",
    "            line_index += 1\n",
    "            continue\n",
    "        try:\n",
    "            fact_triple_extract(sentence, out_file)\n",
    "            out_file.flush()\n",
    "        except:\n",
    "            pass\n",
    "        sentence_number += 1\n",
    "        if sentence_number % 50 == 0:\n",
    "            print(\"%d done\" % (sentence_number))\n",
    "        text_line = in_file.readline()\n",
    "        line_index += 1\n",
    "    in_file.close()\n",
    "    out_file.close()\n",
    "\n",
    "def fact_triple_extract(sentence, out_file):\n",
    "    \"\"\"\n",
    "    对于给定的句子进行事实三元组抽取\n",
    "    Args:\n",
    "        sentence: 要处理的语句\n",
    "    \"\"\"\n",
    "    #print sentence\n",
    "    words = segmentor.segment(sentence)\n",
    "    #print \"\\t\".join(words)\n",
    "    postags = postagger.postag(words)\n",
    "    netags = recognizer.recognize(words, postags)\n",
    "    arcs = parser.parse(words, postags)\n",
    "    #print \"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs)\n",
    "\n",
    "    child_dict_list = build_parse_child_dict(words, postags, arcs)\n",
    "    for index in range(len(postags)):\n",
    "        # 抽取以谓词为中心的事实三元组\n",
    "        if postags[index] == 'v':\n",
    "            child_dict = child_dict_list[index]\n",
    "            # 主谓宾\n",
    "            if child_dict.has_key('SBV') and child_dict.has_key('VOB'):\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                r = words[index]\n",
    "                e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                out_file.write(\"主语谓语宾语关系\\t(%s, %s, %s)\\n\" % (e1, r, e2))\n",
    "                out_file.flush()\n",
    "            # 定语后置，动宾关系\n",
    "            if arcs[index].relation == 'ATT':\n",
    "                if child_dict.has_key('VOB'):\n",
    "                    e1 = complete_e(words, postags, child_dict_list, arcs[index].head - 1)\n",
    "                    r = words[index]\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                    temp_string = r+e2\n",
    "                    if temp_string == e1[:len(temp_string)]:\n",
    "                        e1 = e1[len(temp_string):]\n",
    "                    if temp_string not in e1:\n",
    "                        out_file.write(\"定语后置动宾关系\\t(%s, %s, %s)\\n\" % (e1, r, e2))\n",
    "                        out_file.flush()\n",
    "            # 含有介宾关系的主谓动补关系\n",
    "            if child_dict.has_key('SBV') and child_dict.has_key('CMP'):\n",
    "                #e1 = words[child_dict['SBV'][0]]\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                cmp_index = child_dict['CMP'][0]\n",
    "                r = words[index] + words[cmp_index]\n",
    "                if child_dict_list[cmp_index].has_key('POB'):\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict_list[cmp_index]['POB'][0])\n",
    "                    out_file.write(\"介宾关系主谓动补\\t(%s, %s, %s)\\n\" % (e1, r, e2))\n",
    "                    out_file.flush()\n",
    "\n",
    "        # 尝试抽取命名实体有关的三元组\n",
    "        if netags[index][0] == 'S' or netags[index][0] == 'B':\n",
    "            ni = index\n",
    "            if netags[ni][0] == 'B':\n",
    "                while netags[ni][0] != 'E':\n",
    "                    ni += 1\n",
    "                e1 = ''.join(words[index:ni+1])\n",
    "            else:\n",
    "                e1 = words[ni]\n",
    "            if arcs[ni].relation == 'ATT' and postags[arcs[ni].head-1] == 'n' and netags[arcs[ni].head-1] == 'O':\n",
    "                r = complete_e(words, postags, child_dict_list, arcs[ni].head-1)\n",
    "                if e1 in r:\n",
    "                    r = r[(r.index(e1)+len(e1)):]\n",
    "                if arcs[arcs[ni].head-1].relation == 'ATT' and netags[arcs[arcs[ni].head-1].head-1] != 'O':\n",
    "                    e2 = complete_e(words, postags, child_dict_list, arcs[arcs[ni].head-1].head-1)\n",
    "                    mi = arcs[arcs[ni].head-1].head-1\n",
    "                    li = mi\n",
    "                    if netags[mi][0] == 'B':\n",
    "                        while netags[mi][0] != 'E':\n",
    "                            mi += 1\n",
    "                        e = ''.join(words[li+1:mi+1])\n",
    "                        e2 += e\n",
    "                    if r in e2:\n",
    "                        e2 = e2[(e2.index(r)+len(r)):]\n",
    "                    if r+e2 in sentence:\n",
    "                        out_file.write(\"人名//地名//机构\\t(%s, %s, %s)\\n\" % (e1, r, e2))\n",
    "                        out_file.flush()\n",
    "\n",
    "def build_parse_child_dict(words, postags, arcs):\n",
    "    \"\"\"\n",
    "    为句子中的每个词语维护一个保存句法依存儿子节点的字典\n",
    "    Args:\n",
    "        words: 分词列表\n",
    "        postags: 词性列表\n",
    "        arcs: 句法依存列表\n",
    "    \"\"\"\n",
    "    child_dict_list = []\n",
    "    for index in range(len(words)):\n",
    "        child_dict = dict()\n",
    "        for arc_index in range(len(arcs)):\n",
    "            if arcs[arc_index].head == index + 1:\n",
    "                if child_dict.has_key(arcs[arc_index].relation):\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                else:\n",
    "                    child_dict[arcs[arc_index].relation] = []\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "        #if child_dict.has_key('SBV'):\n",
    "        #    print words[index],child_dict['SBV']\n",
    "        child_dict_list.append(child_dict)\n",
    "    return child_dict_list\n",
    "\n",
    "def complete_e(words, postags, child_dict_list, word_index):\n",
    "    \"\"\"\n",
    "    完善识别的部分实体\n",
    "    \"\"\"\n",
    "    child_dict = child_dict_list[word_index]\n",
    "    prefix = ''\n",
    "    if child_dict.has_key('ATT'):\n",
    "        for i in range(len(child_dict['ATT'])):\n",
    "            prefix += complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "    \n",
    "    postfix = ''\n",
    "    if postags[word_index] == 'v':\n",
    "        if child_dict.has_key('VOB'):\n",
    "            postfix += complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "        if child_dict.has_key('SBV'):\n",
    "            prefix = complete_e(words, postags, child_dict_list, child_dict['SBV'][0]) + prefix\n",
    "\n",
    "    return prefix + words[word_index] + postfix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #extraction_start(in_file_name, out_file_name, begin_line, end_line)\n",
    "    extraction_start(in_file_name, out_file_name, begin_line, end_line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
