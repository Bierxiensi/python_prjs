{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(texts,df['location_id'],test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy, crf_viterbi_accuracy\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Bidirectional, Dropout, LSTM, TimeDistributed, Masking\n",
    "from keras.utils import to_categorical, plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sequence_labeling.utils import event_type\n",
    "from sequence_labeling.utils import MAX_SEQ_LEN, train_file_path, test_file_path, dev_file_path\n",
    "from sequence_labeling.load_data import read_data\n",
    "from albert_zh.extract_feature import BertVector\n",
    "\n",
    "# 使用GPU训练\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7,8\"\n",
    "\n",
    "# 利用ALBERT提取文本特征\n",
    "bert_model = BertVector(pooling_strategy=\"NONE\", max_seq_len=MAX_SEQ_LEN)\n",
    "f = lambda text: bert_model.encode([text])[\"encodes\"][0]\n",
    "\n",
    "# 读取label2id字典\n",
    "with open(\"%s_label2id.json\" % event_type, \"r\", encoding=\"utf-8\") as h:\n",
    "    label_id_dict = json.loads(h.read())\n",
    "\n",
    "id_label_dict = {v:k for k,v in label_id_dict.items()}\n",
    "\n",
    "\n",
    "# 载入数据\n",
    "def input_data(file_path):\n",
    "\n",
    "    sentences, tags = read_data(file_path)\n",
    "    print(\"sentences length: %s \" % len(sentences))\n",
    "    print(\"last sentence: \", sentences[-1])\n",
    "\n",
    "    # ALBERT ERCODING\n",
    "    print(\"start ALBERT encding\")\n",
    "    x = []\n",
    "    pbar = tqdm(sentences)\n",
    "    for i, sent in zip(pbar, sentences):\n",
    "        pbar.set_description(\"Processing bar: \")\n",
    "        x.append(f(sent))\n",
    "\n",
    "    x = np.array(x)\n",
    "    print(\"end ALBERT encoding\")\n",
    "\n",
    "    # 对y值统一长度为MAX_SEQ_LEN\n",
    "    new_y = []\n",
    "    for seq in tags:\n",
    "        num_tag = [label_id_dict[_] for _ in seq]\n",
    "        if len(seq) < MAX_SEQ_LEN:\n",
    "            num_tag = num_tag + [0] * (MAX_SEQ_LEN-len(seq))\n",
    "        else:\n",
    "            num_tag = num_tag[: MAX_SEQ_LEN]\n",
    "\n",
    "        new_y.append(num_tag)\n",
    "\n",
    "    # 将y中的元素编码成ont-hot encoding\n",
    "    y = np.empty(shape=(len(tags), MAX_SEQ_LEN, len(label_id_dict.keys())+1))\n",
    "\n",
    "    for i, seq in enumerate(new_y):\n",
    "        y[i, :, :] = to_categorical(seq, num_classes=len(label_id_dict.keys())+1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Build model\n",
    "def build_model(max_para_length, n_tags):\n",
    "    # Bert Embeddings\n",
    "    bert_output = Input(shape=(max_para_length, 312, ), name=\"bert_output\")\n",
    "    # LSTM model\n",
    "    lstm = Bidirectional(LSTM(units=128, return_sequences=True), name=\"bi_lstm\")(bert_output)\n",
    "    drop = Dropout(0.1, name=\"dropout\")(lstm)\n",
    "    dense = TimeDistributed(Dense(n_tags, activation=\"softmax\"), name=\"time_distributed\")(drop)\n",
    "    crf = CRF(n_tags)\n",
    "    out = crf(dense)\n",
    "    model = Model(inputs=bert_output, outputs=out)\n",
    "    # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(loss=crf.loss_function, optimizer='adam', metrics=[crf.accuracy])\n",
    "\n",
    "    # 模型结构总结\n",
    "    model.summary()\n",
    "    plot_model(model, to_file=\"albert_bi_lstm.png\", show_shapes=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "def train_model():\n",
    "\n",
    "    # 读取训练集，验证集和测试集数据\n",
    "    train_x, train_y = input_data(train_file_path)\n",
    "    dev_x, dev_y = input_data(dev_file_path)\n",
    "    test_x, test_y = input_data(test_file_path)\n",
    "\n",
    "    # 模型训练\n",
    "    model = build_model(MAX_SEQ_LEN, len(label_id_dict.keys())+1)\n",
    "    history = model.fit(train_x, train_y, validation_data=(dev_x, dev_y), batch_size=16, epochs=5)\n",
    "\n",
    "    model.save(\"%s_ner.h5\" % event_type)\n",
    "\n",
    "    # 绘制loss和acc图像\n",
    "    plt.subplot(2, 1, 1)\n",
    "    epochs = len(history.history['loss'])\n",
    "    plt.plot(range(epochs), history.history['loss'], label='loss')\n",
    "    plt.plot(range(epochs), history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    epochs = len(history.history['crf_viterbi_accuracy'])\n",
    "    plt.plot(range(epochs), history.history['crf_viterbi_accuracy'], label='crf_viterbi_accuracy')\n",
    "    plt.plot(range(epochs), history.history['val_crf_viterbi_accuracy'], label='val_crf_viterbi_accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"%s_loss_acc.png\" % event_type)\n",
    "\n",
    "    # 模型在测试集上的表现\n",
    "    # 预测标签\n",
    "    y = np.argmax(model.predict(test_x), axis=2)\n",
    "    pred_tags = []\n",
    "    for i in range(y.shape[0]):\n",
    "        pred_tags.append([id_label_dict[_] for _ in y[i] if _])\n",
    "\n",
    "    # 因为存在预测的标签长度与原来的标注长度不一致的情况，因此需要调整预测的标签\n",
    "    test_sents, test_tags = read_data(test_file_path)\n",
    "    final_tags = []\n",
    "    for test_tag, pred_tag in zip(test_tags, pred_tags):\n",
    "        if len(test_tag) == len(pred_tag):\n",
    "            final_tags.append(test_tag)\n",
    "        elif len(test_tag) < len(pred_tag):\n",
    "            final_tags.append(pred_tag[:len(test_tag)])\n",
    "        else:\n",
    "            final_tags.append(pred_tag + ['O'] * (len(test_tag) - len(pred_tag)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os, re, json, traceback\n",
    "from docx import Document\n",
    "from pyltp import SentenceSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_dir = './corpus/political_news'\n",
    "\n",
    "texts = []\n",
    "file_paths = []\n",
    "subjs, preds, objs = [], [], []\n",
    "\n",
    "total_files = []\n",
    "\n",
    "# 遍历project目录并读取其中的word文档进行SPO提取\n",
    "for root, dirs, files in os.walk(project_dir):\n",
    "    for name in files:\n",
    "        file_path = os.path.join(root, name)\n",
    "\n",
    "        if file_path.endswith('.docx'):\n",
    "\n",
    "            total_files.append(file_path)\n",
    "\n",
    "# 取前100篇文章作为测试\n",
    "# total_files = total_files[:100]\n",
    "\n",
    "bar = tqdm(total_files)\n",
    "\n",
    "for file_path, ch in zip(total_files, bar):\n",
    "\n",
    "    # 输出进度条信息\n",
    "    bar.set_description(\"Processing %s\" % ch)\n",
    "\n",
    "    # 读物word文档内容，并进行分句\n",
    "    document = Document(file_path)\n",
    "    doc_content = ''.join([para.text for para in document.paragraphs])\n",
    "    sents = list(SentenceSplitter.split(doc_content))\n",
    "\n",
    "    # 对每一句话进行SPO提取\n",
    "    for sent in sents:\n",
    "\n",
    "        # 符号替换\n",
    "        # sent = re.sub(r\"(.+?)\", \"\", sent)\n",
    "        sent = re.sub(\"（.+?）\", \"\", sent)\n",
    "        sent = sent.replace(\" \", \"\").replace(\"　\", \"\")\n",
    "\n",
    "        req = requests.post(\"http://localhost:12308/spo_extract\", data={\"text\": sent})\n",
    "        res = json.loads(req.content)\n",
    "\n",
    "        if res:\n",
    "            print(\"\\n原文: %s\" % sent)\n",
    "            print(\"SPO: %s\\n\" % res)\n",
    "\n",
    "            for item in res:\n",
    "                subj = item[\"subject\"]\n",
    "                pred = item[\"predicate\"]\n",
    "                obj = item[\"object\"]\n",
    "\n",
    "                if subj != obj :\n",
    "                    subjs.append(subj)\n",
    "                    preds.append(pred)\n",
    "                    objs.append(obj)\n",
    "                    texts.append(sent)\n",
    "                    file_paths.append(file_path)\n",
    "\n",
    "\n",
    "# 将抽取的三元组结果保存成EXCEL文件\n",
    "df = pd.DataFrame({\"S\": subjs,\n",
    "                   \"P\": preds,\n",
    "                   \"O\": objs,\n",
    "                   \"text\": texts,\n",
    "                   \"file_path\": file_paths\n",
    "                 })\n",
    "\n",
    "# 去除重复行\n",
    "new_df = df.drop_duplicates()\n",
    "\n",
    "# 保存结果\n",
    "new_df.to_excel(\"political_new_extract.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
