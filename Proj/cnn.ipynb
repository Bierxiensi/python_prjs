{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['美日于', '2023', '年', '9', '月', '10', '日', '在', '南海', '举行', '联合', '军演'], ['美韩', '在', '日本海', '举行', '联合', '军演']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (2,) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb 单元格 1\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     vectorized_data\u001b[39m.\u001b[39mappend(fea_sentence(sample_vecs))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m x_train\u001b[39m=\u001b[39mvectorized_data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m y_train\u001b[39m=\u001b[39mdf[\u001b[39m\"\u001b[39m\u001b[39mcountry_id\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32m/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb 单元格 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m n0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0.\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m list_w:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     n0 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m i\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m fe \u001b[39m=\u001b[39m n0 \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(list_w)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W1sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m fe \u001b[39m=\u001b[39m fe\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (2,) (10,) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "#导入Keras中的卷积工具\n",
    "from keras.models import Sequential  #基础的Keras神经网络模型\n",
    "from keras.layers import Dense, Dropout, Activation #模型中常用的层对象\n",
    "from keras.layers import Conv1D,GlobalMaxPooling1D #卷积层和池化\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data.csv')\n",
    "df=df[['text','location', 'country']]\n",
    "\n",
    "#print(\"在 text 列中总共有 %d 个空值.\" % df['text'].isnull().sum())\n",
    "#print(\"在 location 列中总共有 %d 个空值.\" % df['location'].isnull().sum())\n",
    "df[df.isnull().values==True]\n",
    "df = df[pd.notnull(df['text'])]\n",
    "#print(df)\n",
    "\n",
    "df.sample(2)\n",
    "texts = [[word for word in jieba.cut(document)] for document in df['text']]\n",
    "df['country_id'] = df['country'].factorize()[0]\n",
    "df['location_id'] = df['location'].factorize()[0]\n",
    "cat_id_df = df[['country', 'country_id']].drop_duplicates().sort_values('country_id').reset_index(drop=True)\n",
    "cat_id_df = df[['location', 'location_id']].drop_duplicates().sort_values('location_id').reset_index(drop=True)\n",
    "\n",
    "#训练\n",
    "# word_model = word2vec.Word2Vec(texts,vector_size=100,min_count=3,window=6,hs=1)\n",
    "word_model = word2vec.Word2Vec(texts, hs=1,min_count=1,window=6,vector_size=2)\n",
    "word_model.save('word_model')  # 保存模型\n",
    "# word_model = word2vec.Word2Vec.load('word_model')  # 加载模型\n",
    "# print(model.wv.similarity('南海', '日本海'))\n",
    "# print(model.wv['南海'])\n",
    "# for val in model.wv.similar_by_word(\"南海\", topn=10):\n",
    "#     print(val[0], val[1])\n",
    "# for token in texts:\n",
    "#     print(model.wv[token])\n",
    "\n",
    "\n",
    "print(texts)\n",
    "# 直接词向量相加求平均\n",
    "def fea_sentence(list_w):\n",
    "    n0 = np.array([0. for i in range(2)], dtype=np.float32)\n",
    "    for i in list_w:\n",
    "        n0 += i\n",
    "    fe = n0 / len(list_w)\n",
    "    fe = fe.tolist()\n",
    "    return fe\n",
    "\n",
    "# 向量表\n",
    "vectorized_data=[]\n",
    "for text in texts:\n",
    "    sample_vecs = []\n",
    "    for token in text:\n",
    "        try:\n",
    "            sample_vecs.append(word_model.wv[token])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    vectorized_data.append(fea_sentence(sample_vecs))\n",
    "x_train=vectorized_data\n",
    "y_train=df[\"country_id\"]\n",
    "print('x_train', x_train)\n",
    "print('y_train', y_train)\n",
    "print('x_train', np.shape(x_train))\n",
    "print('y_train', np.shape(y_train))\n",
    "\n",
    "\n",
    "# CNN 模型\n",
    "maxlen=400\n",
    "batch_size=32 #在后向传播误差和更新权重前，向网络传入的样本数量\n",
    "embedding_dims=2 #传入卷积神经网络中词条向量的长度\n",
    "filters=250 #要训练的卷积核的数量\n",
    "kernel_size=3 #卷积核大小：每个卷积核将是一个矩阵：embedding_dims*kernel_size\n",
    "hidden_dims=250 #在普通的前馈网络中传播链端点的神经元的数量\n",
    "epochs=1 #整个训练集在网络中的传入次数\n",
    "#定义模型\n",
    "model = Sequential()\n",
    "model.add(Conv1D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    input_shape=(maxlen,embedding_dims)\n",
    "))\n",
    "#池化\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#带dropout的全连接层\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "#漏斗funnel\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "#编译CNN\n",
    "model.compile(loss='sparse_categorical_crossentropy', #多分类的损失函数\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "#categorical变量的输出层\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('sigmoid'))\n",
    "#训练CNN\n",
    "model.fit(\n",
    " x_train,\n",
    " y_train,\n",
    " batch_size=batch_size,\n",
    " epochs=epochs,\n",
    " #  validation_split = 0.2\n",
    " # 训练集的20%用作验证集\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "model_structure=model.to_json()\n",
    "with open(\"cnn_model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights('model_CNN_text.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb 单元格 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     cat_id\u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cat_id_df[cat_id_df\u001b[39m.\u001b[39mcat_id\u001b[39m==\u001b[39mcat_id][\u001b[39m'\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m predict(\u001b[39m\"\u001b[39;49m\u001b[39m衣服真的很大很大，但是颜色好看，字母也很有质感，喜欢喜欢\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m predict(\u001b[39m\"\u001b[39m\u001b[39m美韩在日本海举行联合军演\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb 单元格 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m padded \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(sample_vecs,maxlen\u001b[39m=\u001b[39mmaxlen)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(padded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bierxiensi/Desktop/python_prjs/Proj/cnn.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m cat_id\u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "    df = pd.read_csv('./data.csv')\n",
    "    df=df[['text','location', 'country']]\n",
    "\n",
    "    df[df.isnull().values==True]\n",
    "    df = df[pd.notnull(df['text'])]\n",
    "\n",
    "\n",
    "    df['text_id'] = df['text'].factorize()[0]\n",
    "    cat_id_df = df[['text', 'text_id']].drop_duplicates().sort_values('text_id').reset_index(drop=True)\n",
    "    cat_to_id = dict(cat_id_df.values)\n",
    "    id_to_cat = dict(cat_id_df[['text_id', 'text']].values)\n",
    "\n",
    "\n",
    "    texts = [[word for word in jieba.cut(document)]for document in df['text']]\n",
    "    for txt in texts:\n",
    "        sample_vecs = []\n",
    "        for token in txt:\n",
    "            try:\n",
    "                sample_vecs.append(model.wv[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    padded = np.reshape(sample_vecs,maxlen=maxlen)\n",
    "    pred = model.predict(padded)\n",
    "    cat_id= pred.argmax(axis=1)[0]\n",
    "    return cat_id_df[cat_id_df.cat_id==cat_id]['cat'].values[0]\n",
    "\n",
    "predict(\"衣服真的很大很大，但是颜色好看，字母也很有质感，喜欢喜欢\")\n",
    "predict(\"美韩在日本海举行联合军演\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
